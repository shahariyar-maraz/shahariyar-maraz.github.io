---
layout: about
title: about
permalink: /
subtitle: Research Assistant @ <a href='https://www.maimlab.com/'>University of Dhaka, Bangladesh</a> <br> CS PhD Aspirant

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  more_info:

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit:  # leave blank to include all the news in the `_news` folder

# latest_posts:
#   enabled: false
#   scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
#   limit: 3 # leave blank to include all the blog posts
---

I am a Research Assistant at the [MAIM Lab](https://www.maimlab.com/) of Robotics & Mechatronics Engineering in the University of Dhaka, Bangladesh. Currently, I am working on a [Wellcome Leap - In Utero](https://wellcomeleap.org/inutero/) funded project (Title: Translation of a Wearable Fetal Movement Monitor towards Stillbirth Prevention), under the supervision of [Dr. Abhishek Kumar Ghosh](https://www.du.ac.bd/faculty/faculty_details/RME/2318) and [Dr. Niamh Nowlan](https://people.ucd.ie/niamh.nowlan) to develop a wearable device to monitor fetal movements to cut stillbirth rates.

Back in March '24, I graduated from the University of Dhaka in Robotics and Mechatronics Engineering, supervised by [Dr. Md Mehedi Hasan](https://www.du.ac.bd/faculty/faculty_details/HSS/4706). My fourth year thesis project was on "Enhancing UAV Based Human Action Recognition: A Deep Learning Approach". [`[Report]`](https://drive.google.com/file/d/1im1cmDKfGHdqRqx8z4r-s_zfVFTyEj2g/view?usp=sharing)

---

## **Research Interest**

I'm interested in `Multi-modal Learning`, especially at the fusion of Vision + Language. Besides, I'm also interested in Computer Vision, Embodied AI, and Reinforcement Learning. Overall, I would like to work on fusing complementary intelligence from multi-modalities with the ambitious goal of positively influencing real-world environments via embodied agents.

---
Recently, I submitted my work on language-guided embodied agents to `ICLR '26`. VLMs were used to generate natural language task feedback for causal reasoning via self-reflection and guide the agents for embodied manipulation tasks. 
Prior to that, I've also submitted a manuscript to `AAAI '26` on *Time Series Forecasting* using a tri-modal architecture comprising time, spectral and LLM branches.

I have extensively collaborated on interdisciplinary projects in areas ranging from Machine Learning and Deep learning to Health Informatics and Biomechatronics. My current work revolves around multimodal reasoning for embodied AI, multimodal generation, scene understanding, and vision-language integration.
